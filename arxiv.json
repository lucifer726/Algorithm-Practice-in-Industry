[
    {
        "title": "RouteRAG: Efficient Retrieval-Augmented Generation from Text and Graph via Reinforcement Learning",
        "affiliation": "中国科学院计算技术研究所、中国科学院大学计算机科学与技术学院",
        "url": "http://arxiv.org/abs/2512.09487v1",
        "pub_date": "2025-12-10",
        "summary": "Retrieval-Augmented Generation (RAG) integrates non-parametric knowledge into Large Language Models (LLMs), typically from unstructured texts and structured graphs. While recent progress has advanced text-based RAG to multi-turn reasoning through Reinforcement Learning (RL), extending these advances to hybrid retrieval introduces additional challenges. Existing graph-based or hybrid systems typically depend on fixed or handcrafted retrieval pipelines, lacking the ability to integrate supplementary evidence as reasoning unfolds. Besides, while graph evidence provides relational structures crucial for multi-hop reasoning, it is substantially more expensive to retrieve. To address these limitations, we introduce \\model{}, an RL-based framework that enables LLMs to perform multi-turn and adaptive graph-text hybrid RAG. \\model{} jointly optimizes the entire generation process via RL, allowing the model to learn when to reason, what to retrieve from either texts or graphs, and when to produce final answers, all within a unified generation policy. To guide this learning process, we design a two-stage training framework that accounts for both task outcome and retrieval efficiency, enabling the model to exploit hybrid evidence while avoiding unnecessary retrieval overhead. Experimental results across five question answering benchmarks demonstrate that \\model{} significantly outperforms existing RAG baselines, highlighting the benefits of end-to-end RL in supporting adaptive and efficient retrieval for complex reasoning.",
        "translated": "检索增强生成（RAG）将非参数化知识（通常来自非结构化文本与结构化图谱）整合至大语言模型（LLMs）中。尽管近期研究已通过强化学习（RL）将基于文本的RAG推进至多轮推理阶段，但将这些进展拓展至混合检索仍面临额外挑战。现有基于图谱或混合检索的系统通常依赖固定或人工设计的检索流程，缺乏在推理过程中动态整合补充证据的能力。此外，尽管图谱证据能为多跳推理提供关键的关系结构，但其检索成本显著更高。为应对这些局限，我们提出\\model{}——一个基于强化学习的框架，使大语言模型能够执行多轮自适应的图文混合检索增强生成。该框架通过强化学习联合优化整个生成过程，使模型能够在统一的生成策略中学习何时进行推理、从文本或图谱中检索何种信息，以及何时生成最终答案。为引导这一学习过程，我们设计了兼顾任务结果与检索效率的两阶段训练框架，使模型既能利用混合证据，又能避免不必要的检索开销。在五个问答基准测试上的实验结果表明，\\model{}显著优于现有检索增强生成基线方法，凸显了端到端强化学习在支持复杂推理的自适应高效检索方面的优势。",
        "title_translated": "RouteRAG：基于强化学习的文本与图数据高效检索增强生成"
    },
    {
        "title": "Passing the Baton: High Throughput Distributed Disk-Based Vector Search with BatANN",
        "affiliation": "康奈尔大学、康奈尔大学纽约校区科技学院",
        "url": "http://arxiv.org/abs/2512.09331v1",
        "pub_date": "2025-12-10",
        "summary": "Vector search underpins modern information-retrieval systems, including retrieval-augmented generation (RAG) pipelines and search engines over unstructured text and images. As datasets scale to billions of vectors, disk-based vector search has emerged as a practical solution. However, looking to the future, we need to anticipate datasets too large for any single server. We present BatANN, a distributed disk-based approximate nearest neighbor (ANN) system that retains the logarithmic search efficiency of a single global graph while achieving near-linear throughput scaling in the number of servers. Our core innovation is that when accessing a neighborhood which is stored on another machine, we send the full state of the query to the other machine to continue executing there for improved locality. On 100M- and 1B-point datasets at 0.95 recall using 10 servers, BatANN achieves 6.21-6.49x and 2.5-5.10x the throughput of the scatter-gather baseline, respectively, while maintaining mean latency below 6 ms. Moreover, we get these results on standard TCP. To our knowledge, BatANN is the first open-source distributed disk-based vector search system to operate over a single global graph.",
        "translated": "向量搜索是现代信息检索系统的基石，支撑着检索增强生成（RAG）流程以及面向非结构化文本和图像的搜索引擎。随着数据集规模扩展至数十亿向量，基于磁盘的向量搜索已成为实用解决方案。然而展望未来，我们需要预见到数据集的规模将超出单台服务器的承载极限。本文提出BatANN——一个基于磁盘的分布式近似最近邻（ANN）检索系统，在保持单一全局图对数级搜索效率的同时，实现了随服务器数量接近线性的吞吐量扩展。我们的核心创新在于：当访问存储在其他机器上的邻域时，我们会将查询的完整状态发送至目标机器，使其能在本地继续执行以提升数据局部性。在100M和1B规模数据集上，以0.95召回率为基准，使用10台服务器时BatANN分别实现了6.21-6.49倍和2.5-5.10倍于分散-聚合基线的吞吐量，同时将平均延迟控制在6毫秒以内。值得注意的是，这些成果是在标准TCP协议上实现的。据我们所知，BatANN是首个基于单一全局图实现的开源分布式磁盘向量检索系统。",
        "title_translated": "接力棒传递：基于BATANN的高吞吐量分布式磁盘向量检索"
    },
    {
        "title": "Goal inference with Rao-Blackwellized Particle Filters",
        "affiliation": "佛罗里达大学机械与航空航天工程系、俄亥俄大学数学系",
        "url": "http://arxiv.org/abs/2512.09269v1",
        "pub_date": "2025-12-10",
        "summary": "Inferring the eventual goal of a mobile agent from noisy observations of its trajectory is a fundamental estimation problem. We initiate the study of such intent inference using a variant of a Rao-Blackwellized Particle Filter (RBPF), subject to the assumption that the agent's intent manifests through closed-loop behavior with a state-of-the-art provable practical stability property. Leveraging the assumed closed-form agent dynamics, the RBPF analytically marginalizes the linear-Gaussian substructure and updates particle weights only, improving sample efficiency over a standard particle filter. Two difference estimators are introduced: a Gaussian mixture model using the RBPF weights and a reduced version confining the mixture to the effective sample. We quantify how well the adversary can recover the agent's intent using information-theoretic leakage metrics and provide computable lower bounds on the Kullback-Leibler (KL) divergence between the true intent distribution and RBPF estimates via Gaussian-mixture KL bounds. We also provide a bound on the difference in performance between the two estimators, highlighting the fact that the reduced estimator performs almost as well as the complete one. Experiments illustrate fast and accurate intent recovery for compliant agents, motivating future work on designing intent-obfuscating controllers.",
        "translated": "从移动智能体轨迹的含噪观测中推断其最终目标，是一个基础性的估计问题。本研究首次采用改进型Rao-Blackwellized粒子滤波器（RBPF）进行此类意图推断，其前提假设是：智能体的意图通过具有可证明实际稳定性特征的闭环行为体现。借助假设的闭式智能体动力学模型，RBPF对线性高斯子结构进行解析边缘化处理，仅更新粒子权重，从而在样本效率上超越了标准粒子滤波器。研究提出了两种差分估计器：一种利用RBPF权重构建高斯混合模型，另一种简化版本则将混合模型限制在有效样本范围内。我们通过信息论泄漏度量量化了对手恢复智能体意图的能力，并借助高斯混合模型的KL散度界限，为真实意图分布与RBPF估计之间的Kullback-Leibler（KL）散度提供了可计算的下界。同时，我们给出了两种估计器性能差异的界限，证明简化估计器的表现几乎与完整估计器相当。实验表明，对于合规智能体能够实现快速准确的意图恢复，这为未来设计意图混淆控制器提供了研究动机。",
        "title_translated": "使用Rao-Blackwellized粒子滤波进行目标推断"
    },
    {
        "title": "Meta Lattice: Model Space Redesign for Cost-Effective Industry-Scale Ads Recommendations",
        "affiliation": "Meta AI",
        "url": "http://arxiv.org/abs/2512.09200v1",
        "pub_date": "2025-12-09",
        "summary": "The rapidly evolving landscape of products, surfaces, policies, and regulations poses significant challenges for deploying state-of-the-art recommendation models at industry scale, primarily due to data fragmentation across domains and escalating infrastructure costs that hinder sustained quality improvements.   To address this challenge, we propose Lattice, a recommendation framework centered around model space redesign that extends Multi-Domain, Multi-Objective (MDMO) learning beyond models and learning objectives. Lattice addresses these challenges through a comprehensive model space redesign that combines cross-domain knowledge sharing, data consolidation, model unification, distillation, and system optimizations to achieve significant improvements in both quality and cost-efficiency.   Our deployment of Lattice at Meta has resulted in 10% revenue-driving top-line metrics gain, 11.5% user satisfaction improvement, 6% boost in conversion rate, with 20% capacity saving.",
        "translated": "产品、平台、政策与监管环境的快速演变，对工业级前沿推荐模型的部署构成了严峻挑战。这主要源于跨领域数据碎片化与持续攀升的基础设施成本，两者共同制约着模型质量的持续提升。为应对这一挑战，我们提出Lattice推荐框架——该框架以模型空间重构为核心，将多领域多目标学习范式从模型与目标层面进一步拓展。Lattice通过融合跨领域知识共享、数据整合、模型统一、知识蒸馏与系统优化的综合性模型空间重构方案，在质量与成本效益方面实现显著突破。我们在Meta的实际部署数据显示：Lattice驱动收入的核心指标提升10%，用户满意度提高11.5%，转化率增长6%，同时节省20%的算力资源。",
        "title_translated": "元晶格：模型空间重构，打造高性价比的工业级广告推荐系统"
    },
    {
        "title": "VI-MMRec: Similarity-Aware Training Cost-free Virtual User-Item Interactions for Multimodal Recommendation",
        "affiliation": "香港大学、北京理工大学、都柏林大学、卡内基梅隆大学",
        "url": "http://arxiv.org/abs/2512.08702v1",
        "pub_date": "2025-12-09",
        "summary": "Although existing multimodal recommendation models have shown promising performance, their effectiveness continues to be limited by the pervasive data sparsity problem. This problem arises because users typically interact with only a small subset of available items, leading existing models to arbitrarily treat unobserved items as negative samples. To this end, we propose VI-MMRec, a model-agnostic and training cost-free framework that enriches sparse user-item interactions via similarity-aware virtual user-item interactions. These virtual interactions are constructed based on modality-specific feature similarities of user-interacted items. Specifically, VI-MMRec introduces two different strategies: (1) Overlay, which independently aggregates modality-specific similarities to preserve modality-specific user preferences, and (2) Synergistic, which holistically fuses cross-modal similarities to capture complementary user preferences. To ensure high-quality augmentation, we design a statistically informed weight allocation mechanism that adaptively assigns weights to virtual user-item interactions based on dataset-specific modality relevance. As a plug-and-play framework, VI-MMRec seamlessly integrates with existing models to enhance their performance without modifying their core architecture. Its flexibility allows it to be easily incorporated into various existing models, maximizing performance with minimal implementation effort. Moreover, VI-MMRec introduces no additional overhead during training, making it significantly advantageous for practical deployment. Comprehensive experiments conducted on six real-world datasets using seven state-of-the-art multimodal recommendation models validate the effectiveness of our VI-MMRec.",
        "translated": "尽管现有的多模态推荐模型已展现出良好性能，但其效果仍受普遍存在的数据稀疏性问题制约。该问题源于用户通常仅与可用物品中的一小部分产生交互，导致现有模型将未观测到的物品随意视为负样本。为此，我们提出VI-MMRec——一个与模型无关且无需训练成本的框架，通过相似性感知的虚拟用户-物品交互来增强稀疏的交互数据。这些虚拟交互基于用户交互物品在特定模态特征上的相似性构建而成。具体而言，VI-MMRec引入两种不同策略：（1）叠加策略，独立聚合各模态相似性以保留模态特定的用户偏好；（2）协同策略，整体融合跨模态相似性以捕捉互补的用户偏好。为确保高质量增强，我们设计了基于统计信息的权重分配机制，根据数据集特定的模态相关性自适应地为虚拟交互分配权重。作为即插即用框架，VI-MMRec无需修改现有模型的核心架构即可无缝集成，有效提升其性能。其灵活性使其能够轻松融入各类现有模型，以最小实现成本最大化性能表现。此外，VI-MMRec在训练过程中不引入额外开销，这使其在实际部署中具有显著优势。我们在六个真实数据集上使用七个前沿多模态推荐模型进行的全面实验，验证了VI-MMRec的有效性。",
        "title_translated": "VI-MMRec：多模态推荐中基于相似性感知的无训练成本虚拟用户-物品交互"
    },
    {
        "title": "Ontology-Based Knowledge Graph Framework for Industrial Standard Documents via Hierarchical and Propositional Structuring",
        "affiliation": "汉阳大学、The Miraclesoft",
        "url": "http://arxiv.org/abs/2512.08398v1",
        "pub_date": "2025-12-09",
        "summary": "Ontology-based knowledge graph (KG) construction is a core technology that enables multidimensional understanding and advanced reasoning over domain knowledge. Industrial standards, in particular, contain extensive technical information and complex rules presented in highly structured formats that combine tables, scopes of application, constraints, exceptions, and numerical calculations, making KG construction especially challenging. In this study, we propose a method that organizes such documents into a hierarchical semantic structure, decomposes sentences and tables into atomic propositions derived from conditional and numerical rules, and integrates them into an ontology-knowledge graph through LLM-based triple extraction. Our approach captures both the hierarchical and logical structures of documents, effectively representing domain-specific semantics that conventional methods fail to reflect. To verify its effectiveness, we constructed rule, table, and multi-hop QA datasets, as well as a toxic clause detection dataset, from industrial standards, and implemented an ontology-aware KG-RAG framework for comparative evaluation. Experimental results show that our method achieves significant performance improvements across all QA types compared to existing KG-RAG approaches. This study demonstrates that reliable and scalable knowledge representation is feasible even for industrial documents with intertwined conditions, constraints, and scopes, contributing to future domain-specific RAG development and intelligent document management.",
        "translated": "基于本体的知识图谱构建是实现领域知识多维理解与高级推理的核心技术。工业标准文档尤其包含大量以高度结构化形式呈现的技术信息与复杂规则，其内容往往融合了表格、适用范围、约束条件、例外条款及数值计算，这使得知识图谱构建面临特殊挑战。本研究提出一种方法：首先将此类文档组织为层级化语义结构，进而将句子与表格分解为源自条件规则与数值规则的原子命题，最后通过基于大语言模型的三元组抽取将其整合至本体知识图谱中。该方法同时捕捉了文档的层级结构与逻辑结构，有效表征了传统方法难以反映的领域特定语义。为验证其有效性，我们从工业标准中构建了规则问答、表格问答、多跳问答数据集以及有害条款检测数据集，并实现了具备本体感知能力的KG-RAG框架进行对比评估。实验结果表明，相较于现有KG-RAG方法，本方法在所有问答类型上均取得显著性能提升。本研究表明，即使对于包含交织条件、约束与适用范围的工业文档，可靠且可扩展的知识表示仍然是可行的，这为未来领域特定RAG技术与智能文档管理的发展提供了重要支撑。",
        "title_translated": "基于本体论的知识图谱框架：面向工业标准文档的层次化与命题结构化"
    },
    {
        "title": "ClinicalTrialsHub: Bridging Registries and Literature for Comprehensive Clinical Trial Access",
        "affiliation": "俄亥俄州立大学",
        "url": "http://arxiv.org/abs/2512.08193v1",
        "pub_date": "2025-12-09",
        "summary": "We present ClinicalTrialsHub, an interactive search-focused platform that consolidates all data from ClinicalTrials.gov and augments it by automatically extracting and structuring trial-relevant information from PubMed research articles. Our system effectively increases access to structured clinical trial data by 83.8% compared to relying on ClinicalTrials.gov alone, with potential to make access easier for patients, clinicians, researchers, and policymakers, advancing evidence-based medicine. ClinicalTrialsHub uses large language models such as GPT-5.1 and Gemini-3-Pro to enhance accessibility. The platform automatically parses full-text research articles to extract structured trial information, translates user queries into structured database searches, and provides an attributed question-answering system that generates evidence-grounded answers linked to specific source sentences. We demonstrate its utility through a user study involving clinicians, clinical researchers, and PhD students of pharmaceutical sciences and nursing, and a systematic automatic evaluation of its information extraction and question answering capabilities.",
        "translated": "我们推出ClinicalTrialsHub，这是一个聚焦交互式检索的平台，它整合了ClinicalTrials.gov的所有数据，并通过自动从PubMed研究文献中提取和结构化试验相关信息进行增强。相较于仅依赖ClinicalTrials.gov，我们的系统将结构化临床试验数据的可及性有效提升了83.8%，有望为患者、临床医生、研究人员和政策制定者提供更便捷的访问途径，从而推动循证医学的发展。ClinicalTrialsHub采用GPT-5.1和Gemini-3-Pro等大型语言模型来提升可访问性。该平台自动解析全文研究文献以提取结构化试验信息，将用户查询转化为结构化数据库检索，并提供一个可溯源的问答系统，该系统能生成基于证据的答案，并与具体源语句相链接。我们通过一项涉及临床医生、临床研究人员以及药学与护理学博士研究生的用户研究，以及对其信息提取和问答能力的系统性自动评估，展示了该平台的实用性。",
        "title_translated": "ClinicalTrialsHub：连接注册库与文献，实现临床试验的全面获取"
    },
    {
        "title": "Exploiting the Randomness of Large Language Models (LLM) in Text Classification Tasks: Locating Privileged Documents in Legal Matters",
        "affiliation": "SAP、Ankura Consulting Group、LLC",
        "url": "http://arxiv.org/abs/2512.08083v1",
        "pub_date": "2025-12-08",
        "summary": "In legal matters, text classification models are most often used to filter through large datasets in search of documents that meet certain pre-selected criteria like relevance to a certain subject matter, such as legally privileged communications and attorney-directed documents. In this context, large language models have demonstrated strong performance. This paper presents an empirical study investigating the role of randomness in LLM-based classification for attorney-client privileged document detection, focusing on four key dimensions: (1) the effectiveness of LLMs in identifying legally privileged documents, (2) the influence of randomness control parameters on classification outputs, (3) their impact on overall classification performance, and (4) a methodology for leveraging randomness to enhance accuracy. Experimental results showed that LLMs can identify privileged documents effectively, randomness control parameters have minimal impact on classification performance, and importantly, our developed methodology for leveraging randomness can have a significant impact on improving accuracy. Notably, this methodology that leverages randomness could also enhance a corporation's confidence in an LLM's output when incorporated into its sanctions-compliance processes. As organizations increasingly rely on LLMs to augment compliance workflows, reducing output variability helps build internal and regulatory confidence in LLM-derived sanctions-screening decisions.",
        "translated": "在法律事务中，文本分类模型最常用于筛选大型数据集，以寻找符合特定预设标准的文件，例如涉及特定主题（如法律特权通信和律师指导文件）的相关文档。在此背景下，大型语言模型已展现出卓越的性能。本文通过实证研究探讨了随机性在基于大型语言模型的律师-客户特权文件检测分类中的作用，重点关注四个关键维度：（1）大型语言模型识别法律特权文件的有效性；（2）随机性控制参数对分类输出的影响；（3）随机性对整体分类性能的影响；（4）利用随机性提升准确率的方法论。实验结果表明：大型语言模型能有效识别特权文件；随机性控制参数对分类性能影响甚微；更重要的是，我们所开发的利用随机性的方法能显著提升准确率。值得注意的是，该方法在融入企业制裁合规流程时，还能增强企业对大型语言模型输出的信心。随着各类组织日益依赖大型语言模型来增强合规工作流程，降低输出变异性有助于提升内部及监管机构对基于大型语言模型的制裁筛查决策的信任度。",
        "title_translated": "利用大型语言模型（LLM）的随机性进行文本分类任务：定位法律事务中的特权文件"
    },
    {
        "title": "Leveraging Machine Learning and Large Language Models for Automated Image Clustering and Description in Legal Discovery",
        "affiliation": "Ankura Consulting Group、LLC",
        "url": "http://arxiv.org/abs/2512.08079v1",
        "pub_date": "2025-12-08",
        "summary": "The rapid increase in digital image creation and retention presents substantial challenges during legal discovery, digital archive, and content management. Corporations and legal teams must organize, analyze, and extract meaningful insights from large image collections under strict time pressures, making manual review impractical and costly. These demands have intensified interest in automated methods that can efficiently organize and describe large-scale image datasets. This paper presents a systematic investigation of automated cluster description generation through the integration of image clustering, image captioning, and large language models (LLMs). We apply K-means clustering to group images into 20 visually coherent clusters and generate base captions using the Azure AI Vision API. We then evaluate three critical dimensions of the cluster description process: (1) image sampling strategies, comparing random, centroid-based, stratified, hybrid, and density-based sampling against using all cluster images; (2) prompting techniques, contrasting standard prompting with chain-of-thought prompting; and (3) description generation methods, comparing LLM-based generation with traditional TF-IDF and template-based approaches. We assess description quality using semantic similarity and coverage metrics. Results show that strategic sampling with 20 images per cluster performs comparably to exhaustive inclusion while significantly reducing computational cost, with only stratified sampling showing modest degradation. LLM-based methods consistently outperform TF-IDF baselines, and standard prompts outperform chain-of-thought prompts for this task. These findings provide practical guidance for deploying scalable, accurate cluster description systems that support high-volume workflows in legal discovery and other domains requiring automated organization of large image collections.",
        "translated": "数字图像创建与存储量的快速增长，在法律取证、数字归档和内容管理领域带来了严峻挑战。企业及法律团队必须在严格的时间限制下对大规模图像集进行组织、分析并提取有效信息，这使得人工审查既不现实又成本高昂。这些需求极大地推动了人们对能够高效组织与描述大规模图像数据集的自动化方法的关注。本文通过整合图像聚类、图像描述生成与大语言模型（LLM），对自动化聚类描述生成进行了系统性研究。我们采用K-means聚类算法将图像划分为20个视觉连贯的簇，并利用Azure AI Vision API生成基础描述。随后，我们评估了聚类描述生成过程的三个关键维度：(1) 图像采样策略，对比随机采样、基于质心采样、分层采样、混合采样和基于密度采样与使用全部聚类图像的效果；(2) 提示技术，对比标准提示与思维链提示；(3) 描述生成方法，对比基于LLM的生成与传统TF-IDF及基于模板的方法。我们使用语义相似度和覆盖度指标评估描述质量。结果表明：每簇采用20张图像的战略性采样在显著降低计算成本的同时，其效果与使用全部图像相当，仅分层采样表现略有下降；基于LLM的方法持续优于TF-IDF基线；在此任务中，标准提示的表现优于思维链提示。这些发现为部署可扩展、高精度的聚类描述系统提供了实用指导，可支持法律取证及其他需要自动化组织大规模图像集的领域中的高负荷工作流程。",
        "title_translated": "利用机器学习与大型语言模型实现法律发现中的自动化图像聚类与描述"
    },
    {
        "title": "A Comparative Study of Retrieval Methods in Azure AI Search",
        "affiliation": "Ankura Consulting Group、LLC",
        "url": "http://arxiv.org/abs/2512.08078v1",
        "pub_date": "2025-12-08",
        "summary": "Increasingly, attorneys are interested in moving beyond keyword and semantic search to improve the efficiency of how they find key information during a document review task. Large language models (LLMs) are now seen as tools that attorneys can use to ask natural language questions of their data during document review to receive accurate and concise answers. This study evaluates retrieval strategies within Microsoft Azure's Retrieval-Augmented Generation (RAG) framework to identify effective approaches for Early Case Assessment (ECA) in eDiscovery. During ECA, legal teams analyze data at the outset of a matter to gain a general understanding of the data and attempt to determine key facts and risks before beginning full-scale review. In this paper, we compare the performance of Azure AI Search's keyword, semantic, vector, hybrid, and hybrid-semantic retrieval methods. We then present the accuracy, relevance, and consistency of each method's AI-generated responses. Legal practitioners can use the results of this study to enhance how they select RAG configurations in the future.",
        "translated": "随着律师们寻求超越关键词和语义搜索以提升文件审查任务中关键信息查找效率的需求日益增长，大型语言模型（LLM）正被视为一种新型工具，使律师能够在文件审查过程中通过自然语言提问来获取准确而精炼的答案。本研究评估了微软Azure检索增强生成（RAG）框架中的检索策略，旨在为电子取证领域的早期案件评估（ECA）探索高效方法。在ECA阶段，法律团队会在案件初期分析数据以掌握整体情况，并在启动全面审查前尝试确定关键事实与风险。本文系统比较了Azure AI搜索的关键词检索、语义检索、向量检索、混合检索及混合语义检索五种方法的性能表现，并分析了每种方法生成式AI回答的准确性、相关性与一致性。法律从业者可依据本研究结果，优化未来RAG框架的配置选择策略。",
        "title_translated": "Azure AI搜索中检索方法的比较研究"
    }
]